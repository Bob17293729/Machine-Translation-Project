{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建数据集\n",
    "只抽取训练集中的前 22 万条数据，并从中划分出 2 万条数据作为验证集，然后将 translation2019zh 中的验证集作为测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "\n",
    "max_dataset_size = 220000\n",
    "train_set_size = 200000\n",
    "valid_set_size = 20000\n",
    "\n",
    "class TRANS(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                sample = json.loads(line.strip()) #Sample: dict()\n",
    "                Data[idx] = sample\n",
    "        return Data #A dict(idx) -> a dict(Chinese/English)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "data = TRANS('data/translation2019zh_train.json')\n",
    "train_data, valid_data = random_split(data, [train_set_size, valid_set_size])\n",
    "test_data = TRANS('data/translation2019zh_valid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 200000\n",
      "valid set size: 20000\n",
      "test set size: 39323\n",
      "{'english': '13 emotional drama, I could not act.', 'chinese': '感情的戏，我没演技。'}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理\n",
    "**注意**: 默认情况下分词器会采用源语言(zh)的设定来编码文本，要编码目标语言(en)则需要通过上下文管理器 `tokenizer.as_target_tokenizer()`：\n",
    "模型的输入包括一个字典，关键字含\n",
    "+ `attention_mask`(attention机制0/1)\n",
    "+ `input_ids`(incoder输入的字符编号)\n",
    "+ `labels`(decoder输出字符的编号)\n",
    "+ `decoder_input_ids`(decoder输入的编号，即上一个cell的输出)，直接调用函数`model.prepare_decoder_input_ids_from_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd586beb9ece48d7a2d41b2b9884860e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df428981607c48b8b7407dea7d36cc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838bee20585043158fa3a1680924494b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/807k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa6a2ec53a04db58fd9b9ff35f8b286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baozongbo/opt/anaconda3/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0ddb65002b4e5dab3336773a183c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ce5849394743bca260be52bccfb0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "torch.cuda.empty_cache() #清空缓存\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = model.to(device)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples: #Sample: dict() 套 dict()\n",
    "        batch_inputs.append(sample['chinese']) # [Set_size]\n",
    "        batch_targets.append(sample['english']) # [Set_size]\n",
    "    batch_data = tokenizer( #处理中文\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ) #返回[\"input_ids\"] [\"Attention_mask\"]\n",
    "    with tokenizer.as_target_tokenizer(): #处理英文\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"] #只取[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels) #错位读上一个的输出\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1] #将输出最后一位改为\"<\\s>\"，即-100\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1239,  0.8234],\n",
      "         [-0.4331,  1.0536]],\n",
      "\n",
      "        [[-1.7165,  0.2575],\n",
      "         [ 0.5476,  0.2206]]])\n",
      "(tensor([0, 0, 0, 1, 1, 1]), tensor([0, 0, 1, 0, 1, 1]), tensor([0, 1, 1, 1, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "x =torch.randn(2,2,2)\n",
    "print(x)\n",
    "print(torch.where(x >0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练&验证\n",
    "\n",
    "使用 `AutoModelForSeq2SeqLM` 构造的模型已经封装好了对应的损失函数，并且计算出的损失会直接包含在模型的输出 outputs 中，可以直接通过 outputs.loss 获得\n",
    "\n",
    "评测指标：BLEU，对于中文需手动指定`tokenize='zh'`，英文不需要\n",
    "\n",
    "训练：直接将含四个参数的字典喂进model\n",
    "验证：使用`.generate()`方法喂前两个参数，获得输出idx后再用`.batch_decode()`批量处理两维Tensor，最终得到一个 list\\[ str() \\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.440131:  14%|█▍        | 1/7 [00:00<00:02,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3544, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.321745:  43%|████▎     | 3/7 [00:00<00:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1890, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.240935:  71%|███████▏  | 5/7 [00:01<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3202, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.221717: 100%|██████████| 7/7 [00:01<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0272, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VAL]: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 12.37\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.207874:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1110, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.207874:  14%|█▍        | 1/7 [00:00<00:01,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4743, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.237481:  29%|██▊       | 2/7 [00:00<00:00,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.244335:  43%|████▎     | 3/7 [00:00<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1836, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.238816:  57%|█████▋    | 4/7 [00:00<00:00,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1543, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.231774:  71%|███████▏  | 5/7 [00:00<00:00,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8052, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.183195:  86%|████████▌ | 6/7 [00:01<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TRAIN] loss: 2.183195: 100%|██████████| 7/7 [00:01<00:00,  6.16it/s]\n",
      "[VAL]: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 12.37\n",
      "\n",
      "Epoch 3/8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.000000:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9941, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_num):\n\u001b[0;32m     71\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch_num\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     total_loss \u001b[39m=\u001b[39m train_loop(train_dataloader, model, optimizer, lr_scheduler, t\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, total_loss)\n\u001b[0;32m     73\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(total_loss)\n\u001b[0;32m     74\u001b[0m     valid_bleu \u001b[39m=\u001b[39m test_loop(valid_dataloader, model)\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(trainloader, model, optimizer, lr_scheduler, epoch, total_loss)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\DDA4220\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\DDA4220\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from sacrebleu.metrics import BLEU\n",
    "import numpy as np\n",
    "bleu = BLEU()\n",
    "\n",
    "\n",
    "def train_loop(trainloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(trainloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(trainloader) #上一epoch结束后，最终的batch编号\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(trainloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data) # ** 相当于是将字典内每个部分当成一个参数，输入进model内\n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'[TRAIN] loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def test_loop(valloader, model):\n",
    "    preds, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(valloader, desc='[VAL]'):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy() #验证/测试中，只需要\"input_ids\"及\"attention_mask\"\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True) #模型输出（idx->str）\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True) #数据集输出（idx->str）\n",
    "\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels] #注意:BLEU计算的labels需要以list套list的形式储存\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 8\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler( \n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(train_dataloader)//10,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "train_loss_list, val_loss_list = [], []\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    train_loss_list.append(total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model)\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1229/1229 [46:02<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 0.16\n",
      "\n",
      "saving predicted results...\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "#from sacrebleu.metrics import BLEU\n",
    "import evaluate\n",
    "import numpy as np\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "\n",
    "def test(test_dataloader, model, model_file):    \n",
    "    model.load_state_dict(torch.load(model_file)) #根据具体输出值，读入相应文件\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('evaluating on test set...')\n",
    "        sources, preds, labels = [], [], []\n",
    "        for batch_data in tqdm(test_dataloader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "            ).cpu().numpy() #喂进模型，获得idx\n",
    "            label_tokens = batch_data[\"labels\"].cpu().numpy() #test数据集上的label\n",
    "\n",
    "            decoded_sources = tokenizer.batch_decode(\n",
    "                batch_data[\"input_ids\"].cpu().numpy(), \n",
    "                skip_special_tokens=True, \n",
    "                use_source_tokenizer=True\n",
    "            )# 将原输入的idx解码为label\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True) # 将预测的idx解码为label\n",
    "            label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id) # 将label的替换编号重新改为pad_token_id\n",
    "            decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True) # 将原标签的idx解码为label\n",
    "\n",
    "            ### 储存:\n",
    "            sources += [source.strip() for source in decoded_sources]\n",
    "            preds += [pred.strip() for pred in decoded_preds]\n",
    "            labels += [[label.strip()] for label in decoded_labels]\n",
    "        bleu_score = bleu.compute(predictions=preds, references=labels)[\"bleu\"] #计算得分\n",
    "        print(f\"Test BLEU: {bleu_score:>0.2f}\\n\")\n",
    "        results = []\n",
    "        print('saving predicted results...')\n",
    "        for source, pred, label in zip(sources, preds, labels):\n",
    "            results.append({\n",
    "                \"sentence\": source, \n",
    "                \"prediction\": pred, \n",
    "                \"translation\": label[0]\n",
    "            })\n",
    "        with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "            for exapmle_result in results:\n",
    "                f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n') #将结果输出为json\n",
    "\n",
    "model_file = \"epoch_1_valid_bleu_58.44_model_weights.bin\"\n",
    "test(test_dataloader, model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDA4220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
